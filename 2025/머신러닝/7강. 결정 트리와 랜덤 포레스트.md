# 7강. 결정 트리와 랜덤 포레스트

## 1. 결정 트리

### 결정 트리 decision tree

- 주어진 문제/입력에 관해 결정을 내리는 함수를 트리 형태로 구성
  - 기본적으로 분류 문제를 위해 개발
  - 회귀, CART
- 뛰어난 설명 능력 제공
  - 트리 구조에 각 입력 요소의 역할이 잘 표현되어 학습 결과에 대한 설명 가능
- 과다 적합 문제 발생
  - 복잡한 함수 표현 과정에서 트리 깊이가 깊어져야 하고, 그 과정에서 데이터 노이즈에 민감
  - 앙상블 학습 기법을 결합 → 그게 랜덤 포레스트



### 결정 트리의 학습

- 각 노드에 어떤 속성(결정 요인)을 배정할 것인가?
  - 트리의 레벨을 확장해 나갈 때 확장 횟수가 적을수록 효율적
  - 트리 레벨 확장시 가능한 많은 노드가 리프노드로 할당되는 것이 효과적

- 속성 선택을 위한 평가 기준
  - 지니 불순도(Gini Impurity)
    -  각 노드에 할당된 **클래스 레이블이 얼마나 다른지 그 혼합 정도**를 측정
    - 하나의 그룹 안에서 서로 다른 클래스 레이블이 혼재할 수록 불순도는 높아짐
  - **지니 평가지수(Gini criterion)**
    - 속성 a를 갖는 부모 노드 Ra에서 자식 노드들의 지니 불순도의 가중합

 

### 속성 노드 선택을 위한 그 밖의 평가지수

- **정보 이득(information gain)**
  - 데이터 집합의 분할 전후의 엔트로피의 차이 > 정보 이득이 높은 속성을 기준으로 선택
    - 엔트로피: **주어진 데이터 집합의 혼잡도** → 엔트로피 높고, 정보 이득 낮음
    - 혼잡도 낮은 속성을 선택
- **분산 감소량 variance reduction**
  - 모든 노드에 대한 분산의 가중 평균
    - 분산 → 데이터의 동질성을 표시 → 데이터가 완전히 같으면 분산 0
  - 회귀 문제에 주로 사용
- **Chi-square** 
  - 부모 노드와 하위 노드 간 차이의 통계적 유의성 활용
  - 목표 출력이 범주형인 경우 적합



### 결정 트리를 이용한 분류

- 모든 리프 노드의 지니 불순도가 0이 되기까지의 깊이
- 단, 모든 학습 데이터를 완벽히 분류할 수 있을 때 까지 트리의 깊이를 늘리는 방법으로는 성능이 좋아지지만은 않고, 오히려 데이터에 포함된 노이즈에 민감하게 반응
  - 과다 적합
- 적절한 깊이를 사전에 정해둬야 함



### 회귀를 위한 결정 트리

- 출력값이 해당 영역의 목표값을 가장 잘 근사하는 실수값
  - 근사값은 해당 노드에 포함된 데이터들의 목표 출력값의 평균값
- 과다 적합문제 발생할 수 있음



### 결정 트리의 문제

- 과다 적합 
  - 모든 학습 데이터에 대해 노이즈까지 완벽 학습
- 간단한 해결책
  - 조기종료 - 트리 깊이 조절
  - 가지치키 - 전체 트리 만든 후 불필요한 노드 제거
- 발전된 해결책
  - 랜덤 포레스트



## 2. 랜덤 포레스트

### 랜덤 포레스트?

- 결정 트리와 앙상블 학습 기법을 결합한 방법
  - 배깅 방법으로 데이터를 리샘플링하여 M개의 결정 트리를 학습, 결합
  - 결합 방법 > 분류 문제(주로 보팅), 회귀 문제(출력값의 평균)
  - 포레스트 → M개의 서로 다른 결정 트리가 결합된 형식으로 숲 구조를 가짐
  - 랜덤 → 결정 트리 간의 차이가 랜덤하게 추출된 데이터 샘플에 기인
- 장점
  - 결정트리의 장점
    - 높은 설명 능력, 빠른 학습
  - 앙상블 학습의 장점
    - 간단한 학습기 결합으로 복잡한 함수 표현 및 일반화 성능 향상



### 랜덤 포레스트의 생성 과정

<img src="./assets/Screenshot 2025-11-29 at 3.24.53 PM.png" alt="Screenshot 2025-11-29 at 3.24.53 PM" style="zoom:50%;" />



 