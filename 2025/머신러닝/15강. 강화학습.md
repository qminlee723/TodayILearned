# 15강. 강화학습

## 1. 강화학습의 개요

 ### 머신러닝의 유형

- 지도학습
  - 목표 출력값(교사 supervisor)을 함께 제공
  - 분류, 회귀
- 비지도학습
  - 군집화
- 강화학습(reinforcement learning)
  - 원하는 출력값을 모르거나 알 수 없는 경우
  - 출력값에 대한 교사 신호가 보상 형태
  - 교사 신혼느 정확한 값이 아니고, 출력값 각각에 대해 즉시 주어지지 않을 수 있음



### 강화학습의 적용 사례

- 알파고 Google DeepMind
  - 딥러닝, 강화학습, 몬테카를로 트리 탐색 기술의 결합
  - 완전 자기학습

- 게임
  - Atari 게임
  - 49종 게입
- 다양한 게임에 대한 심층 강화학습의 성능 비교
  - 모든 게임에 대해 하나의 신경망 사용
  - DQN 모델(CNN과 Q-학습의 결합)
- 로봇 제어
  - 모방학습 → 비디오 이미지를 보고 로봇이 흉내 내서 동작을 배우도록 학습
- 시스템 설정/제어
  - 냉각 비용 40% 낮춤



### 강화 학습의 응용 분야

- 보드게임, 비디오 게임, 로봇 제어, 시스템 제어 등
  - 주어진 상황에서 최적의 제어 신호에 대한 정확한 목표값을 주기 힘듦
  - 제어 동작의 수행 결과에 대해 성공 여부의 평가는 가능
- 제어 문제를 표현하고 해결하는 방법으로 주로 사용
  - 주어진 조건이나 상황에서 어떤 동작을 취해야 할 지를 결정하는 문제
  - 특징: 시간에 따른 순차적 개념 존재
    - 이전 상황에 현재 시점에서의 결정에 영향 미침



### 강화 학습

- 출력값에 대한 교사 신호가 보상(reward) 형태로 제공
  - 목표 출력값 없음
    - 최종 목표를 지정하고, 매 순간 학습 시스템이 취하는 행동 결과에 따라 보상 제공
  - 학습의 목적
    - 최종적으로 얻게 되는 누적 보상의 최대화
      - 목표 달성에 적절한(좋은) 행동 → 양의 보상
      - 목표 달성에 부적절한(나쁜) 행동 → 음의 보상
    - 각 순간의 보상을 이용해 좋은/나쁜 행동을 배워서 행동방식을 긍정적인 방향으로 강화(강화학습)



### 강화학습의 표준 계산 모델

- 행동 A, 상태 S, 보상 R의 정의가 필요

- 에이전트의 역할
  - 누적 보상을 최대화하는 최적의 행동 결정



### 상태와 보상의 예

- 상태 집합 S, 행동 집합 A, 보상 신호의 집합 R의 정의 필요 



### MDP 마르코프 결정 프로세스

- MDP, Markov Decision Process
  - 학습을 위해 **에이전트-환경의 상호작용**을 수학적으로 표현한 것
  - 정책 policy → 에이전트가 행동을 선택할 때 사용하는 규칙
    - 에이전트는 정책에 따라 행동을 결정하고, 환경은 주어진 MDP에 따라 다음 상태와 보상을 결정
    - 환경은 주어진 MDP에 따라 다음 상태와 보상을 결정
  - 학습 목적
    - 주어진 MDP에서 누적 보상을 최대화하는 최적의 행동을 결정하는 정책을 찾는 것
  - **마르코프 성질 Markov property**
    - 현재 상태에서 선택할 다음 행동은 **현재의 상태(상황)에 의해서만 결정**된다
    - 이전의 선택/결정들이 영향을 미치지 않음 → **현재 상태**만 판단하면 됨

- < S, A, P, R, 𝛾 >의 튜플
  - S → 가능한 상태의 유한집합
  - A → 가능한 행동의 유한집합
  - P → 상태 전이 확률
    - 상태 s에서 행동 a를 취했을 때 상태 s'로 전이될 확률값
    - 확률적 불확실성이 존재
    - 마르코프 성질을 만족함
  - R → 보상 함수
    - S 에서 행동 a를 취했을 때 얻어질 보상의 기대치
    - 확률적 기대치로서 표현
  - 𝛾 → 보상을 계산할 때 사용되는 할인율(discount factor)(𝛾 ∈ [0,1])



### 미로 찾기 과업을 정의하는 MDP의 예

<img src="./assets/Screenshot 2025-11-30 at 8.51.40 PM.png" alt="Screenshot 2025-11-30 at 8.51.40 PM" style="zoom:50%;" />

- 학습해야 할 내용
  - 어떻게 이동해서, 즉 "어떤 행동들을 선택해서 목표 지점에 도달할 것인가?"



### 정책과 가치함수

- 정책  𝜋
  - 상태와 행동의 시퀀스를 결정하는 함수적 규칙
- 좋은 정책에 대한 평가 기준
  - 수익(return, 이익 gain Gt)
  - 시점 t 에서부터 얻어지는 보상에 대한 할인율 𝛾을 곱해서 더한 값("총할인 보상")
    - total discount reward
  - 가장 근접한 미래에 대한 보상에 가장 큰 가중치를 줌

- 학습의 목적
  - 최종적인 누적 보상을 최대화하는 최적 정책(optimal policy)을 찾는 것
  - 어떻게 최적 정책을 찾을까?
    - 상태와 행동에 대한 가치를 평가하는 함수 사용
- 가치함수
  - 상태 가치함수 state-value function
    - 상태 s에서 시작해서 정책 𝜋에 따라 행동을 취하였을 때 얻을 수 있는 기대 보상(보상의 평균)
  - 행동 가치함수 action-value function
    - 정책 𝜋에 따라 상태 s에서 행동 a를 선택했을 때 얻을 수 있는 기대 보상



### 최적 가치함수와 최적 정책

- 최적 가치함수
  - 최적 상태 가치함수 v*(s)
    - 가능한 모든 정책에 대해 최대값을 갖는 상태 가치함수
  - 최적 행동 가치함수 q*(s,a)
    - 가능한 모든 정책에 대해 최댁밧을 갖는 행동 가치함수
- 최적 정책
  - 최적 행동 가치함수 q*(s, a)를 최대화하는 행동을 찾으면 됨



### MDP에서 학습의 어려움

- 에이전트는 상태와 보상에 관해 오직 지역적/부분적 정보만 관찰/이용 가능
- 보상이 즉시 주어지기보다는 긴 시간 동안 지연 발생
- 상태 전이와 보상이 비결정론적인 경우 함수식으로 정해지지 않거나 알려지지 않은 경우도 존재
- 상태공간과 정책공간이 너무 방대
- 비효율적 학습
  - 심층 Q 학습이 좋은 해결책으로 등장
  - Deep Q-learning



## 2. Q-학습과 심층 Q-신경망

### Q-학습

- 행동 가치함수를 Q 함수라고 함
- Q 학습
  - 최적 정책  𝜋*를 얻기 위해 최적 Q 함수 Q\*(s,a)를 추정하는 방법



### Q 학습 알고리즘

- 각 상태 s 와 행동 a에 대해 근사치 Q(s, a)를 0으로 초기화 (큐햇)

- 임의의 상태 s를 현재 상태로 지정

- 다음 과정을 무한 반복

  - 행동 a를 선택하고 실행

  - 즉각적인 보상 r을 얻음

  - 새로운 상태 s'를 얻음

  - 보상 r과 새로운 상태 s' 정보를 사용해서 근사치 Q(s,a)를 갱신  (큐햇)

    <img src="./assets/Screenshot 2025-11-30 at 9.29.17 PM.png" alt="Screenshot 2025-11-30 at 9.29.17 PM" style="zoom:50%;" /> 



### 4x4 미로 찾기에서 Q-학습의 에

- 결정론적 → 하나의 상태로만 전이 발생
- 상태와 보상 정보는 부분적으로만 관찰됨



### Q-테이블

- Q-값이 이산적이므로 테이블 형태로 표현 가능

  - 16가지 상태 * 4가지 행동

    <img src="./assets/Screenshot 2025-11-30 at 9.31.23 PM.png" alt="Screenshot 2025-11-30 at 9.31.23 PM" style="zoom:50%;" /> 



### Q-학습의 예

<img src="./assets/Screenshot 2025-11-30 at 9.33.26 PM.png" alt="Screenshot 2025-11-30 at 9.33.26 PM" style="zoom:50%;" />

- 갱신 과정

  <img src="./assets/Screenshot 2025-11-30 at 9.33.40 PM.png" alt="Screenshot 2025-11-30 at 9.33.40 PM" style="zoom:50%;" /> 



### 행동 선택을 위한 전략

- 탐험(exploration) vs 탐사(exploitation)
  - 탐험
    - 탐색공간 전체를 골고루 찾음
    - 랜덤하게 다른 행동 선택
  - 탐사
    - 특정한 곳을 중심으로 주변을 집중적으로 찾음
    - 현재의 Q-함수 정보를 바탕으로 최적의 행동을 선택
- 혼합된 선택 전략 → 엡실론 탐욕 𝓔-greedy
  - 시간이 지남에 따라
    - **탐험** 정도는 감소
    - **탐사** 정도는 증가



### 더 좋은 경로에 대한 선택

- 할인된 미래 보상
  - 미래 보상에 할인율 𝛾 적용
- 할인율을 적용한 Q-함수의 갱신 규칙



### 비결정론적 환경

- 결정론적(deterministic) 모델
  - 모델의 출력과 행동이 어떤 임의성도 없이 초기 조건과 파라미터에 의해서만 전적으로 결정
  - 확률성 불확실성이 존재하지 않음
- 비결정론적(non-deterministic) 모델
  - 모델에 내재적인 임의성 존재
  - 동일한 초기 조건과 파라미터 집합일지라도 서로 다른 여러 개의 출력을 생성 가능
  - 예: 장기, 바둑, 체스 등의 보드 게임

- 상태 전이와 Q 함수의 추정치 → 비결정론적
  - 결정론적 버전의 Q-학습이 제대로 동적하지 않음
  - 해결책 → 신뢰요소 ɑ 도입(belief factor)
- ɑ를 가진 Q-학습의 갱신 규칙
  - 비결정론적 환경 모델에서도 사용 가능
  - 신경망에서의 학습률(η)처럼 생각하면 됨



### Q-테이블에서 Q-신경망으로

- Q-테이블의 한계
  - 실제 응용에서의 비효율성
  - 신경망으로 표현(Q-신경망)



### Q-신경망 (Q-Network)

- 신경망을 이용해 Q-함수를 표현하고 추정

  - 기본 형태

    <img src="./assets/Screenshot 2025-11-30 at 9.41.04 PM.png" alt="Screenshot 2025-11-30 at 9.41.04 PM" style="zoom:50%;" /> 

  - 구글 딥마인드가 재정의한 Q-신경망 형태

    <img src="./assets/Screenshot 2025-11-30 at 9.41.23 PM.png" alt="Screenshot 2025-11-30 at 9.41.23 PM" style="zoom:50%;" /> 



### Q-신경망의 학습

- 시점 t에서 학습 파라미터 θ에 대한 Q-신경망의 실제 출력

   <img src="./assets/Screenshot 2025-11-30 at 9.44.15 PM.png" alt="Screenshot 2025-11-30 at 9.44.15 PM" style="zoom:50%;" />

- 목표 출력값

- 목적 함수



### Q-신경망 학습의 불안정성

- 목적함수
  - Q-테이블 표현에 대해서는 수렴, Q-신경망에서는 수렴이 보장되지 않음
- 불안정성의 원인
  - 데이터 간의 높은 상관 관계
  - 목표 출력값이 시간에 따라 변하는 시변성
- 해결책
  - DQN Deep Q-Network
    - 심층 신경망
    - 경험 재현 experience replay, 목표망 target-network



### Deep Q-Network(DQN)

<img src="./assets/Screenshot 2025-11-30 at 9.45.39 PM.png" alt="Screenshot 2025-11-30 at 9.45.39 PM" style="zoom:50%;" />

- 아타리 게임을 위한 신경망 구조



### 경험 재현, 목표망

- 경험 재현 → 데이터 간의 높은 상관관계에 따른 문제 해결

  - 재현을 통해 학습 데이터의 시퀀스를 재구성

    - 에이전트의 경험을 시간 간격 단위로 재현 메모리 D에 저장후, D로부터 균등 무작위 추출을 통해 미니 배치를 구성해 학습 진행

  - 목표망 →  시변적인 목표 출력값 문제 해결

    - 원래 Q-신경망과 같은 구조 파라미터 θ-를 가진 별도의 신경망

      <img src="./assets/Screenshot 2025-11-30 at 9.47.24 PM.png" alt="Screenshot 2025-11-30 at 9.47.24 PM" style="zoom:50%;" /> 

  - 목적함수

    <img src="./assets/Screenshot 2025-11-30 at 9.47.44 PM.png" alt="Screenshot 2025-11-30 at 9.47.44 PM" style="zoom:50%;" /> 

- 성공적으로 학습됨



### DQN 학습 알고리즘

<img src="./assets/Screenshot 2025-11-30 at 9.48.37 PM.png" alt="Screenshot 2025-11-30 at 9.48.37 PM" style="zoom:50%;" />

<img src="./assets/Screenshot 2025-11-30 at 9.48.57 PM.png" alt="Screenshot 2025-11-30 at 9.48.57 PM" style="zoom:50%;" />

