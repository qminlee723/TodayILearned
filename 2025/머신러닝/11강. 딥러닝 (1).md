# 11강. 딥러닝 (1)

## 1. 딥러닝의 등장

### MLP에서 심층 신경망으로

- 딥러닝
  - 심층 신경망 기반의 머신러닝 분야
  - deep vs shallow
- 심층 신경망의 기본 형태
  - 많은 수의 은닉층을 가진 MLP
  - 장점
    - 더 효율적 표현 가능
  - 단점
    - 학습 어려움
    - 느린 수렴 속도, 낮은 일반화 성능

- 학습의 어려움을 극복하게 만든 요인
  - 충분히 **큰 학습 데이터 집합** 사용
  - **컴퓨터 파워의 향상**과 **GPU**를 활용하는 병렬 처리 기술 적용
  - 성능 향상을 위한 다양한 **학습 기법** 개발
  - **CNN과 LSTM**과 같은 정교하고 효율적 모델 등장

- 심층 신경망 사용하면 종단간 학습 가능(end-to-end learning)
  - 특징추출 과정과 특징에 의한 분류 과정을 한꺼번에 학습



## 2. 학습 성능 향상을 위한 기법

### 지역극소 local minima

- 기울기 강하 학습법의 근본적인 문제

  - 오차가 충분히 작지 않은 지역 극소에서 학습이 멈춤 > 학습 실패

  - 회피 대안
    - simulated annealing → 학습률을 처음에는 크게, 차차 줄여나감
    - 확률적 기울기 강하(온라인 학습) → 한 번에 하나의 샘플만 학습



### 느린 학습

#### (1) 플라토 문제(Plateau problem)/기울기 소멸 문제

- 플라토 문제

  - 플라토: 기울기 강하 학습의 오차함수의 학습곡선에서 평평한 구간
  - 오차함수에는 플라토를 만드는 안장점이 무수히 많이 존재
    - 안장점: 극대/극소가 아닌 극점(미분값=0)

- 기울기 소멸 문제

  - 가중치 수정폭은 기울기의 크기에 의존
  - 출력층으로부터 오차 신호가 거꾸로 전파되어 입력층으로 내려오면서 점점 약해져서 학습이 느려지거나 진행되지 않는 현상

  <img src="./assets/Screenshot 2025-11-30 at 2.34.05 PM.png" alt="Screenshot 2025-11-30 at 2.34.05 PM" style="zoom:40%;" /> <img src="./assets/Screenshot 2025-11-30 at 2.35.03 PM.png" alt="Screenshot 2025-11-30 at 2.35.03 PM" style="zoom:50%;" />

  - 셀 포화(cell saturation) 구간
    - 시그모이드 함수의 미분값이 거의 0이 됨
    - 미분값.. 1보다 작은 값을 계속 곱하면 점점 작아져서 0과 계속 가까워짐 - 결국 가중치 수정이 제대로 안이루어짐



### 느린 학습의 개선 기법

#### (1) 활성화 함수의 변화

<img src="./assets/Screenshot 2025-11-30 at 2.40.19 PM.png" alt="Screenshot 2025-11-30 at 2.40.19 PM" style="zoom:50%;" />

- 기울기 소멸 문제 → 활성화 함수 기울기가 작아져서 발생
- sigmoid, tanh 함수 대신 기울기가 줄어들지 않는 함수 사용해 해결
  - ReLU, softplus, leaky ReLU, PReLU 등



#### (2) 가중치 초기화

- 입력의 가중합 u가 좋은 범위에 있도록(셀 포화가 일어나지 않도록) 설정

  - 각 뉴런의 가중치가 서로 다르도록 **작은 값**으로 **랜덤**하게 설정

    

#### (3) 모멘텀

- 기울기 강하 학습법의 기울기 수정식

  <img src="./assets/Screenshot 2025-11-30 at 2.42.03 PM.png" alt="Screenshot 2025-11-30 at 2.42.03 PM" style="zoom:50%;" />

- 모멘텀항 → 이전의 움직임(관성)을 반영

  - 학습 속도의 저하를 방지하거나 학습의 불안정성 감소

- 변형 → NAG(Nesterov Accelerated Gradient)

  <img src="./assets/Screenshot 2025-11-30 at 2.42.50 PM.png" alt="Screenshot 2025-11-30 at 2.42.50 PM" style="zoom:50%;" /> 



#### (4) 적응적 학습률(η)

<img src="./assets/Screenshot 2025-11-30 at 2.45.35 PM.png" alt="Screenshot 2025-11-30 at 2.45.35 PM" style="zoom:50%;" />

- 가중치마다 서로 다른 학습률 사용
  - 가중치가 변화된 크기의 누적합을 이용해 변화폭이 큰 가중치는 감소시키는 등의 방식으로 변화폭에 따라 학습률을 적응적으로 조정
  - 대표적 방법 → RMSProp, AdaDelta, **Adam**
    - Adam(Adaptive Momentum) → RMSProp과 모멘텀 방법의 결합



#### (5) 배치 정규화(batch normalization)

- 학습하는 동안 각 노드의 활성화 함수로 들어가는 데이터 배치에 대한 입력이 셀 포화되지 않는 범위 내 있도록 강제적으로 정규화하는 방법
- 활성화 함수에 대한 입력 분포를 항상 일정하게 유지 → 학습 효율 향상



#### (6) 2차 미분 방법

- 기울기 변화량 결정시 오차함수의 2차 미분인 곡률(curvature) 정보를 함께 활용

  <img src="./assets/Screenshot 2025-11-30 at 2.47.19 PM.png" alt="Screenshot 2025-11-30 at 2.47.19 PM" style="zoom:50%;" /> 

- 이론적으로는 좋지만, 계산이 느려서 실질적 사용에는 한계

  - 작은 모델에서 빠르고 정확한 학습이 요구되는 경우 사용

 

### 과다 적합

- 학습 데이터에 포함된 노이즈까지 학습하게 돼 새롭게 주어진 테스트 데이터에 대해 정확도/성능이 떨어지는 현상
  - 가중치 개수가 많아지면 신경망 복잡도가 높아짐
    - 표현 효율은 향상되지만, 과다 적합이 발생할 가능성이 높아짐



### 과다 적합의 해결책 

#### (1) 조기 종료 early stopping

- 검증용 데이터 집합에 대한 검층오차를 에포크마다 확인해 과다적합이 생기기 전에 학습 종료 시점 결정



#### (2) 정규항 추가

- 오차함수에 정규항 추가

  - 오차함수를 줄이는게 목표, 가중치를 작게 가져가서 과다적합 막음
  - 학습하는 동안 가중치가 지나치게 커지는 것을 막음

  <img src="./assets/Screenshot 2025-11-30 at 3.25.15 PM.png" alt="Screenshot 2025-11-30 at 3.25.15 PM" style="zoom:50%;" /> 

   

#### (3) 드롭아웃 dropout

<img src="./assets/Screenshot 2025-11-30 at 3.27.13 PM.png" alt="Screenshot 2025-11-30 at 3.27.13 PM" style="zoom:50%;" />

- 학습 과정에서 가중치를 수정할 때 임의로 선택한 은닉 노드의 일부를 제외하는 것
- 전체 모델이 가지는 복잡도보다는 낮은 모델로 학습
- 작은 모델의 앙상블 평균과 유사한 효과 → 일반화 성능 향상



#### (4) 데이터 증대 data augmentation

- 원래 데이터에 인위적인 변형을 가해 추가적인 데이터 생성 → 충분한 학습 데이터 확보
- 데이터 변형 방법
  - 일반 데이터의 경우 노이즈 추가
  - 영상 데이터의 경우 크기 조정, 회전, 위치 이동, 자르기 등





## 3. 합성곱 신경망(CNN)

### 정교화된 심층 신경망 모델의 등장

- CNN, 합성곱 신경망
  - Convolutional Neural Networks
  - 인간의 시각 피질에서의 정보 처리 기제로부터 영감을 받은 모델
  - 격자 구조를 가진 영상 데이터 처리에 적합한 모델
- RNN, 순환 신경망
  - 기존 RNN의 발전된 모델 → LSTM, GRU 등
  - 음성, 텍스트와 같은 시계열 데이터 처리에 적합



### 합성곱 신경망

- **신경세포** → 3가지 유형(층)
  - 콘볼루션 / convolution
  - 서브샘플링/풀링 subsampling/pooling
  - 완전연결 fully connected
- **네트워크 구조** → 층상 구조
  - 입력층 → 2D 격자 구조 x 다중 채널
  - 콘볼루션층, 풀링층 → 2D 특징맵 x 다중 필터 filter, plane, kernel
    - 부분적인 연결 local connection, 가중치 공유 shared weight
  - 완전연결층 → MLP 구조
- **학습 알고리즘**
  - 오류 역전파 알고리즘 + ɑ



### 콘볼루션층

<img src="./assets/Screenshot 2025-11-30 at 3.39.02 PM.png" alt="Screenshot 2025-11-30 at 3.39.02 PM" style="zoom:50%;" />

- 주어진 2D 입력에 콘볼루션 연산을 반복 적용해 특징맵을 생성
  - 콘볼루션 연산 →  해당 위치의 요소들에 가중치를 곱해서 모두 더하는 선형 연산
  - 필터 w가 학습 대상이 되는 가중치로, 모든 가중치를 공유함

- 패딩 zero padding
  - 입력 데이터의 가장자리를 0으로 채움
    - 필터 크기에 따라 패딩의 크기도 달라짐

- 2차원 격자 입력이 다중 채널을 형성하느 ㄴ경우
  - 입력 영상이 RGB 컬러 영상의 경우
- 다양한 형태의 특징을 추출하려는 경우
  - 다수의 필터 사용 →  다수의 특징맵 생성

- 보폭(stride)

  <img src="./assets/Screenshot 2025-11-30 at 3.43.40 PM.png" alt="Screenshot 2025-11-30 at 3.43.40 PM" style="zoom:50%;" />

  - 필터가 움직이는 간격의 조정을 통해 특징맵의 크기 조정 가능

- 필터의 크기가 1x1인 경우 → 입력과 출력은 동일한 크기

  <img src="./assets/Screenshot 2025-11-30 at 3.44.47 PM.png" alt="Screenshot 2025-11-30 at 3.44.47 PM" style="zoom:50%;" />

  - 다중 채널의 입력에 대해 다중 필터를 적용하는 경우 → 데이터 차원 축소 효과

- 콘볼루션 연산의 간략한 표현

    <img src="./assets/Screenshot 2025-11-30 at 3.47.40 PM.png" alt="Screenshot 2025-11-30 at 3.47.40 PM" style="zoom:50%;" />

- 왜 콘볼루션인가?

  - 실제 뇌의 시각 피질에서 영감 받음
  - 원래 영상에서 의미 있는 특징을 추출하려면
    - 기존 수작업에 의한 설계가 아닌, CNN에서는 학습을 통해 추출



### 풀링층(서브샘플링 층)

- 풀링 연산

  <img src="./assets/Screenshot 2025-11-30 at 3.50.45 PM.png" alt="Screenshot 2025-11-30 at 3.50.45 PM" style="zoom:50%;" />

  - 최대 풀링 max pooling
  - 평균 풀링

- 사용자 정의 파라미터

  <img src="./assets/Screenshot 2025-11-30 at 3.51.03 PM.png" alt="Screenshot 2025-11-30 at 3.51.03 PM" style="zoom:50%;" /> 

  - 풀링 연산을 수행하더라도, 특징맵 갯수는 그대로임

- 왜 풀링인가?

  - 특징 맵의 크기를 작게 만듦 → 계산 속도 향상, 정보의 추상화
  - 데이터의 작은 위치 이동 변화를 수용 transition-invariant



### CNN의 예: LeNet-5

- 1988 Yann LeCun 필기 숫자 인식(99.05) → CNN의 첫 성공 사례

