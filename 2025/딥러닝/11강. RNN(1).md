# 11강. RNN(1)

## 1. RNN

### 1. 시퀀스(sequence)

- 항목(item)들의 나열
  - 문장: 단어 나열
  - 쇼핑 목록: 구매 상품 나열
  - 음악: 음표 나열
  - 경제지표의 시계열 데이터
- 데이터에 예측에 필요한 충분한 정보가 있는가
- 모델이 데이터의 패턴 학습할 수 있는 역량을 가지는가



### 2. RNN의 개념

- RNN(Recurrent Neural Network)

  <img src="./assets/Screenshot 2025-12-05 at 10.52.28 PM.png" alt="Screenshot 2025-12-05 at 10.52.28 PM" style="zoom:50%;" />



### 3. RNN의 유형

- 입력: 시퀀스, 출력: 시퀀스
  - 번역

- 입력: 시퀀스, 출력: 벡터

  - 감정 분석(sentiment analysis)

- 입력: 벡터, 출력: 시퀀스

  - 이미지를 입력으로 받아서, 해당 이미지를 설명하는 문장 생성

- 양방향 RNN(bi-directional RNN)

  - 특정 시점의 추론을 위해 해당 시점 앞뒤의 정보가 모두 필요한 경우

  <img src="./assets/Screenshot 2025-12-05 at 10.54.26 PM.png" alt="Screenshot 2025-12-05 at 10.54.26 PM" style="zoom:50%;" />

  - merge 방식
    - <img src="./assets/Screenshot 2025-12-05 at 10.55.02 PM.png" alt="Screenshot 2025-12-05 at 10.55.02 PM" style="zoom:50%;" /> 
    - 두 벡터를 이어붙이거나(concatenate) 더할(sum) 수 있음



### 4. RNN의 학습

- RNN을 펼쳐놓은 채로 다층 퍼셉트론에서 했던 것과 똑같은 역전파 방식을 적용
  - BPTT(BackPropagation Through Time)



## 2. LSTM & GRU

### 1. LSTM의 개념

- RNN의 문제
  - 긴 시퀀스 다루기 어렵다
  - 정보를 기억하는데 사용되는 은닉 상태 벡터의 차원이 한정되어 있어, 시퀀스의 길이가 길어지면 앞쪽의 정보가 뒤쪽으로 전달되면서 정보의 손실이 일어나 모델 성능이 떨어지게 됨
  - 앞쪽의 내용을 잊어버리고, 최근 내용만 기억하도록 내버려 두는 것이 아니라, 상대적으로 중요한 내용을 보존하고 덜 중욯나 내용을 잊어버리도록 하여 기억 공간을 효과적으로 활용해야 함
- LSTM(Long Short-Term Memory)
  - 중요한 정보와 잊어버려도 좋은 정보를 모델이 학습하도록 함
  - 기존 은닉 상태 벡터를 장기기억 벡터 c와 단기 기억 벡터 h로 분리
  - 목적을 달성하기 위해 일정의 필터인 게이트 도입
    - 망각(forget)
    - 입력(input)
    - 출력(output)

### 2. LSTM의 구조

- RNN의 은닉 상태 부분이 LSTM 셀로 교체됨

<img src="./assets/Screenshot 2025-12-05 at 10.58.48 PM.png" alt="Screenshot 2025-12-05 at 10.58.48 PM" style="zoom:50%;" />

- LSTM 셀에서 수행되는 연산
  - <img src="./assets/Screenshot 2025-12-05 at 11.00.50 PM.png" alt="Screenshot 2025-12-05 at 11.00.50 PM" style="zoom:50%;" /> 3개의 게이트 모두 입력으로 받는 정보가 동일함
  - <img src="./assets/Screenshot 2025-12-05 at 11.01.08 PM.png" alt="Screenshot 2025-12-05 at 11.01.08 PM" style="zoom:50%;" /> 
    - 시그모이드 함수를 활성함수로 적용
    - <img src="./assets/Screenshot 2025-12-05 at 11.01.55 PM.png" alt="Screenshot 2025-12-05 at 11.01.55 PM" style="zoom:50%;" /> 

-  LSTM 셀에서 수행되는 연산

  <img src="./assets/Screenshot 2025-12-05 at 11.02.44 PM.png" alt="Screenshot 2025-12-05 at 11.02.44 PM" style="zoom:50%;" /> 



### 3. GRU(Gated Recurrent Unit)

- LSTM의 단순화 버전
  - 장기기억 벡터와 단기기억 벡터를 별도로 유지하지 않고, RNN처럼 **하나의 상태 벡터만 유지**하도록 변경
  - 망각 게이트와 입력 게이트를 별도로 유지하지 않고, **하나의 게이트가 두 가지 역할을 동시**에 수행하도록 함
  - **출력 게이트를 삭제**



### 4. LSTM과 GRU의 의의

- 더 긴 시퀀스를 다룰 수 있게 만들자
  - LSTM과 GRU이후에도 어텐션(attention) 등과 같이 모델이 더 긴 시퀀스를 다룰 수 있게 하려는 노력이 계속되는데, 더 큰 시퀀스를 다룰 수 있으면 어떤 것이 유리한가?
    - 모델이 예측에 더 많은 정보를 활용할 수 있음
  - **트랜스포머(Transformer)**가 등장하면서, RNN, LSTM 등은 잘 사용되지 않게 되었음
    - 디테일 기억보다 아이디어 이해에 의의가 있음