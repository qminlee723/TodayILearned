# 4강. 딥러닝의 학습 기술(1)

## 1. 최적화와 경사 하강법

### 1. 경사 하강법의 개념

- 딥러닝의 목적
  - 설계한 모델이 가장 바람직한 결론을 내릴 수 있도록 학습표본 집합을 이용해 모델 내부의 파라미터가 최적의 값이 되게 조정하는 훈련을 하는 것
- 최적화 문제(optimization problem)
  - 목적함수(objective function)를 최적화하는 파라미터를 결정하는 문제
    - 딥러닝: 훈련 데이터 집합에 대한 손실함수(loss function)을 최소화하는 문제

- 볼록 최적화(convex optimization)
  - 목적함수가 볼록함수(convex function)이고 해를 찾기 위한 정의역이 볼록집합(convex set)인 최적화 문제
- 경사 하강법(gradient descent)
  - 적절히 선택된 초깃값으로부터 시작해, 경사를 따라 이동해 목적함수의 최솟값에 해당되는 지점인 x*에 도달하는 방법(최저점)
  - 목적함수가 볼록함수가 아니면, 경사 하강법이 최적화 성공 보장 어려움
    - 지역 최소치에 멈춰서, 전역 최소치에 도달 못함
    - 고원(plateau)
- 경사 하강법을 이용한 볼록함수의 최적화
  - 실함수인 목적함수 J가 최소가 되는 파라미터 x* 구하기
  - 초깃값에서 시작 > J 경사의 음의 방향으로 이동
  - 학습률 에타 - 너무 크면 해에 수렴하지 못하고 튕겨져 나갈 수도 있고.. 적절한 값 결정 필목
- 경사 하강법의 문제점
  - 목적함수가 볼록함수가 아니면, 해의 탐색에 실패할 수 있음
    - 모든 파라미터에 대한 목적함수의 편미분이 0이지만, 전역 최소치에 해당되지 않는 파라미터 값인 경우
    - 최역 최소치(local minima)
    - 안장점(saddle point) - 근처가 평평한 경우가 많이 있음. 



### 2. 경사 하강법의 구현

- **배치 경사 하강법(Batch Gradient Descent, 배치 GD)**

  - **모든** 훈련용 표본으로 한 단계의 파라미터 업데이트를 위한 경사를 계산하는 방식

    - 경사 구할 때 모든 표본에 대한 경사를 구한 다음, 평균을 구한 다음, 이를 가지고 파라미터를 업데이트 

    - 하나의 에폭(epoch)에 파라미터 한 번 업데이트 됨
    - 한 번의 업데이트에 긴 계산 시간을 소비함

- **확률적 경사 하강법(Stochastic Gradient Descent, SGD)**

  - 훈련 집합에서 **무작위 순서**로 하나의 표번에 대한 경사를 계산해 파라미터를 업데이트 함

  - 하나의 에폭동안 N번의 업데이트됨

    - 배치 방식에 비해 매우 빠르게 파라미터 업데이트가 진행됨

  - 무작위로 표본 선택, 표본 단위로 파라미터가 업데이트 됨

    - 배치 경사 하강법에 비해 파라미터의 변화가 불규칙하게 진행됨

      - 단층 피드포워드 신경망

      - 보라색이 배치 경사 하강법, 주황색이 확률적 경사 하강법

        <img src="./assets/Screenshot 2025-12-03 at 12.10.47 AM-4688255.png" alt="Screenshot 2025-12-03 at 12.10.47 AM" style="zoom:50%;" /> 

    - 지역최소치, 안장점 등에서 빠져나오는 데 도움이 될 수 있음

  - 극소점 근처에 도달한 상태에서도 파라미터가 계속해 변화

    - 최적의 파라미터로 수렴하지 않을 수 있음
    - 동적 학습률 적용: 에폭에 따라 학습률을 점차 작은 값으로 줄임

- **미니배치 확률적 경사 하강법(mini-batch SGD)**

  - 전체학습표본을 '미니배치'라고 하는 작은 크기의 부분집합으로 분할해 모델 훈련

    - 각각의 미니배치는 학습표본 집합에서 무작위로 선택함
    - 파라미터의 업데이트는 미니배치 단위로 함

  - 배치 경사 하강법에 비해 빠르게 파라미터 업데이트 진행이 가능

    - 하나의 에폭에 파라미터를 (N/Nb)회 업데이트 가능

  - SGD에 비해 파라미터 업데이트의 불규칙성이 완화되어 최적값에 가깝게 파라미터 값이 결정될 수 있음

    - 빨간색이 미니 배치

    <img src="./assets/Screenshot 2025-12-03 at 12.14.19 AM.png" alt="Screenshot 2025-12-03 at 12.14.19 AM" style="zoom:50%;" /> 

  - 계산 성능을 높일 수 있음

    - 특히 GPU를 사용하는 경우, 미니배치 단위의 처리를 하면 행렬 연산의 최적화에 유리함

- 텐서플로(Keras)에서 경사 하강법의 구현



### 3. 동적 학습률

- 학습의 진척에 따라 점차 학습률을 줄이는 방식
  - **계단형 감쇠 스케쥴러**
    - 반복 횟수의 **구간**을 정해 각 구간에 정해진 학습률을 적용
    - `tf.keras.optimizers.schedules` 모듈의 `PiecewiseConstnatDecay` 클래스 인스턴스 활용 
  - **지수함수 감쇠 스케쥴러**
    - 반복 횟수에 따라 초깃값으로부터 **지수함수 형태**로 감쇠함
    - `tf.keras.optimizers.schedules` 모듈의 `ExponentialDecay` 클래스 인스턴스 활용
  - **다항식 감쇠 스케쥴러**
    - 반복 횟수의 다항식 함수에 의해 감쇠
    - `tf.keras.optimizers.schedules` 모듈의 `PolynomialDecay` 클래스 인스턴스 활용
    - 음의 함수로도 갈수도있으므로, 텐서플로우에서는 범위정함



## 2. 심층 신경망의 학습 문제

### 1. 불안정한 경사(unstable gradient) 문제

- 경사 소멸(vanishing gradient) 문제

  - 심층망을 학습하는 과정에서, 입력층으로 갈수록 경사의 크기가 0에 근접해 연결 가중치의 업데이트가 진행되지 않는 현상

  - 시그모이드와 같은 활성함수 사용시 두드러짐

    <img src="./assets/Screenshot 2025-12-03 at 12.23.53 AM.png" alt="Screenshot 2025-12-03 at 12.23.53 AM" style="zoom:50%;" /> 

  - 경사 소멸 문제의 개선 방법

    <img src="./assets/Screenshot 2025-12-03 at 9.26.32 AM.png" alt="Screenshot 2025-12-03 at 9.26.32 AM" style="zoom:50%;" /> 

    - 시그모이드 > 활성함수 개선(ReLU) > 

  - 스킵 연결

    - 경사 소멸 문제를 어느정도 개선

  - 가중치의 적절한 초기화

  - 배치 정규화

- 경사 폭발(exploding gradient) 문제

  - 경사가 점점 더 큰 값을 가져 연결 가중치가 발산
  - 원인
    - 부적절한 가중치 초깃값
    - 지나치게 높은 학습률
  - 개선 방법
    - 배치 정규화
    - 경사 절단(gradient clipping)
    - 규제(regularization)
    - 최적화 알고리즘의 개선(Adma, RMSprop 등)



### 과적합 문제

- 특정 학습 데이터 집합에 지나치게 의존적으로 학습
  - 일반화의 오류 발생
- 개선 방법
  - 드롭아웃 규제
  - 데이터 증강 등



## 3. 가중치 초기화

### 1. 사전 학습에 의한 가중치 초기화

- 심층 신뢰망(deep belief nets, DBN)을 이용한 가중치 초기화

  - Geoffrey Hinton(2006):  연결 가중치를 단순히 랜덤 값으로 초기화하지말고, 적절한 방식으로 초기화함으로써 성능 개선

    - 심층 신뢰망을 **사전 학습** 해서 연결 가중치 초기화(DBN)
    - (+) 여기에다가 피드포워드(판별기) 

    <img src="./assets/Screenshot 2025-12-03 at 9.33.41 AM.png" alt="Screenshot 2025-12-03 at 9.33.41 AM" style="zoom:50%;" /> 



### 2. Glorot 초기화 방법

- Xavier Glorot, Yoshua Bengio(2010)

  - 연결 가중치를 뉴런의 팬-인(fan-in)과 팬-아웃(fan-out)에 따라 결정되는 값의 범위에 속하는 랜덤 값으로 초기화
  - Keras의 초기화를 위한 모듈인 `initializers`에 2가지 유형의 초기화기 제공

- `GlorotUniform`

  - [-limit, limit] 범위의 균등분포로 초깃값 선택

- `GlorotNormal`

  - 평균이 0, 표준 편차가 시그마인 정규분포로 초깃값 선택

    <img src="./assets/Screenshot 2025-12-03 at 9.37.22 AM.png" alt="Screenshot 2025-12-03 at 9.37.22 AM" style="zoom:50%;" /> 



### 3. He 초기화 방법

- Kaiming He, et al(2016)

  - ReLU 유형의 활성함수를 사용하는 신경망에 적합한 초기화 방법을 제안
  - 팬-인을 바탕으로 해 정해지는 랜덤 값에 따라 가중치 초기화
  - Keras 초기화를 위한 모듈인 `initializers` 에 2가지 유형의 초기화기 제공

- `HeUniform`

  - [-limit, limit]

- `HeNormal`

  - 평균이 0, 표준 편차가 시그마인 정규분포로 초깃값 선택

    <img src="./assets/Screenshot 2025-12-03 at 9.37.57 AM.png" alt="Screenshot 2025-12-03 at 9.37.57 AM" style="zoom:50%;" /> 

