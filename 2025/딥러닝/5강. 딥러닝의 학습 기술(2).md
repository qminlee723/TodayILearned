# 5강. 딥러닝의 학습 기술(2)

## 1. 최적화기의 개선

### 1. 모멘텀 및 넨스테로프 가속 경사(NAG)

- 모멘텀

  - 이전 업데이트 양('속도')을 일정 비율('모멘텀') 반영하는 방법

- 네스테로프 가속 경사(Nesterov Accelerated Gradient, NAG)

  ```tensorflow
  from tensorflow.keras import optimizers
  optimizer = optimizers.SGD(0.1, momentum=0.9, nesterov=True)
  ```



### 2. Adagrad와 RMSProp

<img src="./assets/Screenshot 2025-12-03 at 9.19.20 PM.png" alt="Screenshot 2025-12-03 at 9.19.20 PM" style="zoom:50%;" />

- Adagrad

  - 학습률을 적응적으로 적용하기 위한 최적화 방법
  - 변화가 큰 파라미터의 학습률은 작게, 변화가 작은 파라미터의 학습률은 크게 함으로써 파라미터의 변화가 극소점을 향해 진행되게 함. 
  - Si(t)가 점점 커져서 학습률이 매우 작아지게 됨(거의 0에 근접)

- RMSProp(Root Mean Square Propagation)

  - Adagrad에서 학습률이 지나치게 작아지는 문제를 방지
  - 지수함수적으로 감쇠하도록 누적

  ```python
  optimizer = optimizers.RMSprop(0.01, rho=0.9)
  ```



### 3. Adam(Adaptive Momentum estimation)

- 모멘텀과 RMSProp을 결합한 최적화 알고리즘
  - 1차 모멘트의 추정치 m: 경사의 이동평균
  - 2차 모멘트의 추정치 v: 경사 제곱의 이동평균

- 텐서플로우 Adam 최적화기 사용 예문

  ```python
  optimizer = optimizers.Adam(0.01, beta_1=0.9, beta_2=0.99)
  ```

  



## 2. 과적합과 규제

### 1. 과적합과 일반화 오류

<img src="./assets/Screenshot 2025-12-03 at 9.21.38 PM.png" alt="Screenshot 2025-12-03 at 9.21.38 PM" style="zoom:50%;" />

- 과적합(overfitting)
  - 훈련에 사용한 데이터에 지나치게 적합하게 학습하게 되는 것
  - 규제(regularization): 모델 복잡도를 낮춰서 더 단순한 모델로 만들기 위한 처리 절차



### 2. 훈련의 조기 종료(early stopping)

<img src="./assets/Screenshot 2025-12-03 at 9.22.46 PM.png" alt="Screenshot 2025-12-03 at 9.22.46 PM" style="zoom:50%;" />

- 모델 학습의 검증을 위한 학습 데이터 집합의 활용

- 텐서플로에서 조기종료

  - 조기 종료를 위한 콜백 인스턴스 생성
    - Monitor - 어느 항목을 모니터링할 것인가
    - min_delta - 최소한의 변화(0이면 - 나빠지면 조기종료됨)
    - Patience - 한번 성능이 나빠졌다고 해서 바로 끝내지 말고, 경향이 파악되면 종료하자 - 10이라고 하면 10번까지는 봐주는거
    - mode - 증가하는 것에 대해 주의할 것이냐, 감소하는 것에 대해 주의할 것인지를 정하는 것. auto로 설정하면 monitor 값에 따라 자동으로 설정. (loss면 높아지면 주의 - 정확도면 낮아지면 주의)
    - Restore_best_weights - 가장 성능이 좋았던 weight값을 기억해 놓고, 만약 성능이 안좋아지면 해당 가중치로 복귀할 수 있게 하는 것

  ```python
  from tensorflow.keras import callbacks
  early_stop = callbacks.EarlyStopping(monitor='val_loss',
                                      min_delta=0,
                                      patience=0,
                                      mode='auto',
                                      restore_best_weights=False)
  ```

  - fit 메소드에 조기 종료 콜백 지정
  
    ```python
    model.fit(tr_data, tr_labels, epochs=1000,
    					callbacks=[early_stop])
    ```
  
    

### 3. 가중치 규제

-  가중치 감쇠(weight decay)

  - 신경망의 가중치가 작은 값을 갖도록 억제함으로써 네트워크의 복잡도에 제한을 가하는 것

  - Adam W: 가중치 감쇠를 포함하는 Adam 최적화의 변형

- ℓ1, ℓ2 규제
  - 최적화를 위한 목적함수에 손실함수와 더불어 가중치 w의 크기에 따른 불이익(penalty) 항을 추가하는 규제 방법
    - w의 크기카 크면 불이익 항의 크기가 커짐
    - 목적함수 값이 커지므로, w크기가 작아지도록 최적화가 진행됨

- ℓ2 규제가 많이 사용됨

  - <img src="./assets/Screenshot 2025-12-03 at 9.34.25 PM.png" alt="Screenshot 2025-12-03 at 9.34.25 PM" style="zoom:50%;" /> 

  - 기본적인 SGD를 사용하는 경우 (ℓ2 규제 == 가중치 감쇠), RMSProp, Adam 등에서는 ℓ2 규제와 가중치 감쇠는 다름

- 텐서플로에서 ℓ2 규제 적용

  ```python
  from tensorflow.keras import layers, regularizers
  d_layer = layers.Dense(3, activation='relu',
                        kernel_regularizer=regularizers.L2(0.01))
  ```



### 4. 드롭아웃

<img src="./assets/Screenshot 2025-12-03 at 9.37.06 PM.png" alt="Screenshot 2025-12-03 at 9.37.06 PM" style="zoom:50%;" />

- 드롭아웃(dropout)의 개념

  - 모델을 학습하는 동안, 적절한 확률에 따라 뉴런을 무작위로 선택해 일시적으로 제거

- 드롭아웃의 적용

  - 적용 대상 층: 은닉층 및 입력층
  - 드롭아웃 비율(dropout rate): 뉴런을 드롭아웃할 확률을 나타내는 값 p
    - 10-50% 범위에서 지정하는 것이 일반적
    - 드롭아웃 대상 뉴런의 선정: 매 훈련 단계마다 무작위로 선정
  - 훈련단계: 드롭아웃이 적용된 층으로부터 입력받을 때 1/(1-p)를 곱해 전체적인 신호 크기 유지
  - 추론단계: 드롭아웃 없이 모든 뉴런이 동작

- 텐서플로아웃에서 드롭아웃 적용

  - `tf.keras.layers` 모듈의 Dropout 층 배치

  ```python
  from tensorflow import keras
  from tf.keras import Sequential, layers
  model = Sequential([keras.Input(shape=(10,)),
                     layers.Dropout(rate=0.2),
                      layers.Dense(32, activation='relu'),
                      layers.Dropout(rate=0.2),
                      layers.Dense(4, activation='softmax')])
  ```

  



## 3. 배치 정규화

### 1. 배치 정규화의 개념

- 내부 공변량 변화(internal covariate shift)
  - 학습 중 각 층의 활성함수 입력 분포가 이전 층의 파라미터 변경에 의해 변화하는 현상
  - 심층망의 학습 효율이 낮아지는 문제의 원인
    - 작은 학습률을 사용해야 해서 학습이 느림
    - 파라미터 초기화를 주의 깊게 할 필요 있음
    - 포화 비선형 활성함수를 사용하는 모델의 학습이 어려움
  - Sergey loffe 등(2015): 배치 정규화(batch normalization)를 통해 학습 성능 개선 가능

- 정규화(normalization)
  - 입력의 분포가 평균이 0, 분산이 1이 되도록 만드는 것

- 스케일 및 이동 변환
  - 정규화만 할 경우, 그 층에서 표현하려는 정보에 변화가 발생
  - Sigmoid의 선형 영역에만 국한됨 > 이런것들을 막기 위해서 



### 2. 배치 정규화 변환 - 학습 단계

<img src="./assets/Screenshot 2025-12-03 at 9.46.49 PM.png" alt="Screenshot 2025-12-03 at 9.46.49 PM" style="zoom:67%;" />

### 3. 배치 정규화 변환 - 추론 단계

- 학습을 마친 모델을 이용한 추론
  - 추론 시에는 미니배치가 아닌 개별 입력에 대한 출력을 구해야 함
  - 전체 학습표본 집합에 대해 추정한 평균 및 분산을 이용해 입력을 정규화한 후 출력을 구함



### 4. 텐서플로에서 배치 정규화의 적용

- `tf.keras.layers`에 제공되는 `BatchNormalization` 클래스

```python
from tensorflow import keras
from tensorflow.keras import Sequential, layers
model = Sequential([keras.Input(shape=(10,)),
                   layers.Dense(32, activatoin='relu'),
                   layers.Dense(4, activation='softmax')])
```

```python
from tensorflow import keras
from tensorflow.keras import Sequential, layers
model = Sequential([keras.Input(shape=(10,)),
                   layers.Dense(32),
                   layers.BatchNormalization(), # 학습
                   layers.ReLU(),
                   layers.Dense(4, activation='softmax')])
```

