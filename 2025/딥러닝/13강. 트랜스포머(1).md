# 13강. 트랜스포머(1)

## 1. 트랜스포머의 개요

### 1. 트랜스포머(Transforemer)의 등장

- 2017년 구글 연구팀에 의해 제안
- 번역 태스크에 적용
- RNN 문제 해결 장점
- CNN이 컴퓨터 비전 분야의 혁신을 가져왔다면, 트랜스포머가 **자연언어처리 분야**의 혁신을 가져옴
- 이후 등장한 초거대 언어 모델들은 트랜스포머 기반으로 구현



### 2. RNN의 문제

- 계산의 병렬화(parallelization)가 불가능
  - t 시점의 출력은 t-1 시점의 값에 의존적이므로, 순차적으로 계산해야 할 수 없음
  - 학습 역시 BPTT(Backpropagation Through Time) 방식으로 진행할 때, 최종 출력으로부터 시점을 거슬러 올라오면서 순차적으로 경사(gradient)를 계산해야 함
- 시퀀스 길이 증가에 따른 성능 감소



### 3. RNN 대비 트랜스포머의 장점

- 계산의 병렬화 가능
  - n개의 토큰으로 이루어진 입력을 각 층에서 n개의 벡터로 변환한다고 했을 때, 각 벡터의 계산이 서로에게 의존적이지 않음
- 더 긴 시퀀스 다룰 수 있게 됨
  - 셀프 어텐션(self-attention) 매커니즘의 도입



### 4. 트랜스포머의 구조

<img src="./assets/Screenshot 2025-12-05 at 11.51.37 PM.png" alt="Screenshot 2025-12-05 at 11.51.37 PM" style="zoom:50%;" />

- 인코더-디코더 구조
  - 인코더
    - 입력으로 들어온 단어 수 만큼의 벡터를 출력해서 디코더로 전달
  - 디코더
    - 인코더에서 넘어온 정보와 입력을 종합해서 결과 생성
  - RNN 대신 **셀프 어텐션 구조**가 인코더 및 디코더에서 사용됨



## 2. 트랜스포머의 세부구조

### 1. 위치 인코딩

<img src="./assets/Screenshot 2025-12-05 at 11.53.44 PM.png" alt="Screenshot 2025-12-05 at 11.53.44 PM" style="zoom:50%;" />

- 단어 임베딩 + 위치 정보

  - 같은 단어도 문장 내 위치에 따라 역할 및 의미가 달라질 수 있음

  - 단어 임베딩에 **위치를 나타내는 함수값을 더해줌**으로써 **위치 정보**를 추가

    - positional encoding

      <img src="./assets/Screenshot 2025-12-05 at 11.54.00 PM.png" alt="Screenshot 2025-12-05 at 11.54.00 PM" style="zoom:50%;" /> 

- 위치 인코딩 계산의 예

  <img src="./assets/Screenshot 2025-12-05 at 11.55.57 PM.png" alt="Screenshot 2025-12-05 at 11.55.57 PM" style="zoom:50%;" /> 



### 2. 셀프 어텐션

- 기존 어텐션과의 차이
  - 어텐션
    - 어떤 부분이 중요한지를 판단하는 일종의 필터
  - 크로스 어텐션
    - 인코더와 디코더 사이에서 작용
  - 셀프 어텐선
    - 같은 텍스트 안의 단어들 사이에서 작용

- 기존 어텐션
  - 인코더의 은닉상태벡터를 가중평균해 문맥벡터를 계산
  - 가중치는 디코더의 은닉상태벡터와 인코더의 은닉상태벡터의 점곱
- 셀프 어텐션
  - V의 가중평균으로 출력을 생성
  - 가중치는 Q와 K의 점곱
  - V, Q, K는 입력에 행렬(=학습해야 하는 파라미터)를 곱해서 생성

- 셀프 어텐션의 계산

  <img src="./assets/Screenshot 2025-12-06 at 1.51.29 AM.png" alt="Screenshot 2025-12-06 at 1.51.29 AM" style="zoom:50%;" /> 

  - 트랜스포머의 각 층은 입력으로 들어온 벡터와 같은 수 / 같은 차원의 벡터를 출력으로 내보냄
  - 따라서 벡터의 차원을 맞춰줄 필요가 있음

- 멀티 헤드 어텐션(Multi-Head Attention)

  <img src="./assets/Screenshot 2025-12-06 at 2.00.39 AM.png" alt="Screenshot 2025-12-06 at 2.00.39 AM" style="zoom:50%;" />

  - 셀프 어텐션이 문장 내에서 중요한 특징을 뽑아내는 추출기라면, 이런 추출기를 여러 개 적용하면 서로 다른 다양한 특징들이 뽑혀 나올 수 있음
  - Wq, Wk, Wv를 하나씩 두는게 아니라 헤드의 수만큼 둬서 어텐션을 여러 개 계산
    - 헤드 수가 8개면, Wq, Wk, Wv가 각각 8개씩 존재
    - a1, a2도 각각 8개씩 생성
    - a들을 이어붙인 뒤(16차원 벡터) Wo(4 * 16 행렬)를 곱해서 결국 동일한 차원의 o1, o2가 만들어짐
      - 4차원 벡터가 되어야 함



### 3. Add & Normalization

- Add
  - 잔차 연결(residual connection)을 적용
  - 상대적으로 짧은 경로를 추가해서 경사 소멸(vanishing gradient) 문제 경감
  - 원래 입력과 멀티 헤드 어텐션의 출력 사이의 잔차에 대해 학습
  - 정보 소실이 일어날 수 있으므로, 원래 입력을 다시 더해줘서 정보 보존
- 계층 정규화(Layer Normalization)
  - 배치 정규화와 같이 딥러닝 학습을 안정적으로 만들기 위한 장치의 일종
  - 배치 정규화는 RNN계열의 모델에서는 그다지 효과가 없었음



### 4. Feed Forward

- 완전연결층 2번 적용
  - Add & Norm 층의 출력 각각에 완전연결층을 적용



### 5. 인코더

- 다음 단계들을 N번 반복
  - Multi-Head Attention
  - Add & Norm
  - Feed Forward
  - Add & Norm



### 6. 디코더

- Masked Multi-Head Attention
  - Decoder 훈련시 각 단어 위치의 뒤쪽에 대해 셀프 어텐션을 계산하게 되면, 정답을 엿보는 셈이므로 각 단어 자신 및 앞쪽에 대해서만 셀프 어텐션을 수행하도록 뒤쪽에 대해서는 마스킹 적용
  - 뒤쪽을 보지 않고 앞쪽만 보고 그 다음을 인과적으로 판단하므로, 인과 셀프 어텐션(causal self attention)이라고도 함
- 크로스 어텐션
  - 어텐션을 계산하는 식은 셀프 어텐션의 경우와 동일하지만, 키 k와 값 v는 인코더로부터 넘어온 값으로 만들고, 쿼리 q는 디코더의 Add & Norm 층에서 나온 출력으로부터 생성



### 7. 학습 및 추론

- 모델의 학습
  - 소스(source) 텍스트와 타겟 텍스트를 한 번에 주고, 병렬적으로 학습
  - 단, 마스킹 이용해 정답 못엿보게 함
- 추론(=번역)
  - 인코더에 소스 텍스트, 디코더에 [START] 토큰 입력 => 'I' 예측(실제로는 각 단어에 대한 확률 분포를 계산 후 샘플링)
  - 디코더에 [START], 'I' 입력(인코더 출력은 재활용) => 'am' 예측
  - 디코더에 [START], 'I', 'am' 입력 => 'a' 예측



### 8. 트랜스포머 구조가 만드는 특성

- 계산의 병렬화

  - 셀프 어텐션 계산 과정을 보면, a2 계산시 a1이 필요하지 않고 그 역도 마찬가지
  - 매 층마다 출력값 각각을 별도로 계산 가능
  - 토큰의 순서 정보를 위치 인코딩을 통해 데이터에 추가한 다음, 각 토큰에 대한 벡터들을 서로에 대한 의존 없이 별도로 계산 가능
  - 병렬적으로 계산 가능, 리소스 절약, 효율적, 모델도 오버피팅 없이 가능

- 트랜스포머와 RNN의 최대 경로 길이 비교

  <img src="./assets/Screenshot 2025-12-06 at 2.15.37 AM.png" alt="Screenshot 2025-12-06 at 2.15.37 AM" style="zoom:50%;" />

  - RNN은 O(n)인데 비해, 트랜스포머의 경우 셀프 어텐션에 의해 모든 단어와 단어 사이 연결이 존재하므로, 시퀀스의 길이 n과 상관 없이 모든 단어 사이가 직접 연결(O(1))이 된다
  - RNN의 경우, n이 커질수록 정보가 전달될 때 거쳐야 하는 최대 경로의 길이가 선형적으로 늘어나기때문에, 긴 시퀀스 다루는데 한계 있음
  - 트런스포머는 더 긴 시퀀스를 잘 다룰 수 있음 

- 셀프 어텐션의 직관적 의미와 장점
  - 시퀀스에서 모델의 목적에 필요한 부분에 더 집중 가능
  - 시퀀스 내 모든 단어 사이 연결이 존재하므로, 멀리 떨어진 단어들 사이의 의존 관계 파악 가능하며, 결과적으로 RNN에 비해 긴 시퀀스 다룰 수 있음
  - 계산을 병렬적으로 할 수 있어, RNN에 비해 더 짧은 시간에 많은 데이터 학습 가능



### 9. 트랜스포머 모델의 평가

- 기존 SOTA(State of the art) 모델보다 상대적으로 **적은 훈련 비용에 비해 좋은 결과** 냄

  <img src="./assets/Screenshot 2025-12-06 at 2.17.40 AM.png" alt="Screenshot 2025-12-06 at 2.17.40 AM" style="zoom:50%;" />

  