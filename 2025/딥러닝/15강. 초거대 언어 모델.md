# 15강. 초거대 언어 모델

## 1. 언어 모델

### 1. 언어 모델

- 단어들의 등장 확률을 예측하는 모델
  - 어떤 단어들이 등장하면, 그럴듯한지에 대한 확률 분포를 예측
  - 확률 분포를 가지고 단어를 반복적으로 추출해 텍스트를 **만들어 냄**
    - 생성형 AI
- n-gram 모델
  - 언어모델의 타입
  - n-1개의 단어 다음에 올 n번째 단어를 예측하는 모델



### 2. NPLM(Neural Probabilistic Language Model)

<img src="./assets/Screenshot 2025-12-06 at 2.24.13 AM.png" alt="Screenshot 2025-12-06 at 2.24.13 AM" style="zoom:50%;" />

- 신경망을 이용해 확률 분포를 예측하는 모델
  - 신경망을 사용해 구현한 n-gram 모델
  - 단어들의 id가 임베딩으로 변환된 뒤, 은닉층 하나를 거쳐 마지막에는 단어 수만큼의 확률 계산
  - 학습에 사용된 단어들의 시퀀스 길이 n=3~5



## 2. GPT

### 1. GPT & BERT

- GPT(Generative Pre-trained Transformer, OpenAI, 2018)
  - 트랜스포머 중 **디코더** 부분만을 사용해 만든 n-gram 모델 학습
  - n-gram 형태의 모델은 결국 앞쪽의 결과로 뒤쪽을 맞추게 하는 것이기 때문에 디코더의 **인과 셀프 어텐션** 적용 
- BERT(Bidirectional Encoder Representations from Transformers, Google, 2018)
  - 트랜스포머 중 **인코더** 부분만을 사용해 만든 언어 모델 학습
  - 문장 곳곳을 랜덤하게 마스크로 가려 놓은 문장 A, B를 주고
    - 가려놓은 부분의 단어와
    - B가 A 다음에 이어지는 문장이 맞는지 맞추도록 모델 학습



### 2. 전이 학습(transfer learning)

- 사전 학습: 미세 조정과 같이 2단계로 학습을 진행하는 방식
  - 기존에 해당 분야의 일반적 데이터에 대해 사전 학습을 시킨 모델을 가져와서
  - 실제로 해결하려는 문제의 세부 분야 데이터를 추가적으로 학습시키게 되면
  - 기존 모델의 파라미터가 세부 분야에 맞게 미세 조정되면서 좋은 성능을 내게 됨
    - 영어라는 언어에 대한 이해 → 영화 리뷰에 대한 감정 분석
- 사전 학습(pre-training)
  - 문제가 속한 분야에 대한 일반적 이해를 높이는 학습
  - 소스 태스크(source task) 또는 업스트림 태스크(upstream task)에 대해 모델을 학습
    - n-gram 모델을 학습함으로써 언어에 대해 일반적인 이해를 가지게 됨
- 미세 조정(fine-tuning)
  - 문제를 해결하고 싶은 세부 분야에 맞게 사전 학습된 모델 조정
  - 타깃 태스크(target task) 또는 다운스트림 태스크(downstream task)에 대해 모델 학습
    - 텍스트 분류, 감정 분석(sentiment analysis), 텍스트 사이의 유사성(similarity) 판별, 문제에 대한 답 찾기(question-answering) 등
- GPT와 BERT 모두 전이 학습을 통해 세부 태스크를 수행하기 위한 모델 생성



### 3. GPT의 구조

- 미세 조정시 태스크별로 모델 구조를 다르게 구성

  <img src="./assets/Screenshot 2025-12-06 at 2.33.03 AM.png" alt="Screenshot 2025-12-06 at 2.33.03 AM" style="zoom:50%;" />



### 4. GPT의 학습

- 사전 학습
  - **디코더 12개를 쌓은** 트랜스포머 모델에 n-1개 토큰을 입력으로 주고 n번째 토큰을 예측하도록 학습
  - 사전 학습을 위해 사람이 레이블을 매길 필요가 없고, **대량의 텍스트**만 확보하면 됨 → 자기지도학습(self-supervised learning)
  - BookCorpus라는 데이터셋(8억개 정도의 단어들로 구성)과 영문 위키피디아의 텍스트를 사전 학습에 사용
- 미세 조정
  - 태스크에 맞는 최종 형태의 텐서를 만들어 내는 Linear 층으로 기존 Linear층을 교체
  - 세부 태스크 학습을 위한 데이터셋을 주고 모델 학습
  - 미세 조정 때 최종 출력에서 가까운 Linear층은 초깃값으로부터 파라미터 변화가 크게 일어나지만, 더 아래층으로 내려갈수록 파라미터 변화의 정도가 덜하게 되고, 언어에 대해 모델이 가진 기본적인 지식을 보존하는 역할을 하게 됨
- 학습 방식 및 결과
  - **멀티 헤드 어텐션**이 사용되었고, **12개의 헤드**를 사용
  - GPT가 학습한 **최대 시퀀스 길이 n=512**
  - 시퀀스를 이루는 단위로 단어가 아니라 BPE(Byte Pair Encoding)을 사용
    - BPE란 **단어의 일부(subword)를 토큰**으로 사용하는 것
    - 텍스트를 분석해 공통적으로 많이 나타나는 형태를 추출
  - GPT는 몇몇 자연어처리 태스크에서 대부분의 SOTA(State-Of-The-Art) 모델보다 좋은 성능을 보임



## 3. GPT-2

### 1. 아이디어

- 별도의 미세 조정 없이 언어 모델 하나로 여러 가지 태스크를 잘 하게 만들 수 없을까?
  - 언어 모델은 주어진 텍스트 뒤에 올 자연스러운 텍스트를 생성해 낼 수 있는 능력 가짐
  - 이와 같은 점을 이용하면, 언어 모델이 여러 가지 자연언어처리 태스크를 수행가능
    - 번역
    - 요약
  - GPT-2 에서는 태스크별 미세 조정 단계가 존재하지 않음



### 2. GPT-2의 구조

- 기본적으로 GPT와 같은 방식의 모델 사용
  - n-gram 모델
  - 24개의 디코더
  - **계층 정규화(Layer norm) 부분의 위치**가 어텐션과 피드포워드 부분의 앞쪽으로 변경
  - 최대 시퀀스 길이 n=1024



### 3. 실험 방식

- 모델 크기를 달리해 성능 관찰
  - 파라미터 수가 1억개 정도인 모델부터, 최대 15억개인 모델까지 실험 수행



### 4. 실험 결과

<img src="./assets/Screenshot 2025-12-06 at 2.41.45 AM.png" alt="Screenshot 2025-12-06 at 2.41.45 AM" style="zoom:50%;" />

- 4가지 태스크에 대해 평가
  - GPT-2 하나로 여러 분야 태스크 수행 가능
  - 대체로 SOTA 모델의 성능은 못 넘어섰으나, 모델의 파라미터 수가 늘어남에 따라 성능 올라가는 경향 보임
    - DrQA + PGNet이 SOTA 모델



## 4. GPT-3

### 1. 아이디어

- GPT-2 실험에서 모델의 크기가 커짐에 따라 성능이 증가하는 경향 보임. 모델을 더 크게 키우면 어떨까?
  - 15억개 → 1,750억 개 파라미터



### 2. GPT-3의 구조

- 기본적으로 GPT-2와 동일
  - 미세 조정 없이 트랜스포머 디코더를 여러 층 쌓아 올린 언어 모델 하나로 여러 가지 태스크 수행
  - 희소 어텐션이라는 것을 기존 어텐션과 섞어 사용
- 희소(sparse) 어텐션
  - 기존 디코더의 인과 셀프 어텐션은 쿼리가 자신 앞쪽의 모든 키에 대해 어텐션 계산
  - 이 경우 시퀀스 길이 n에 대해 연결 수는 O(n^2)의 복잡도 가지게 되고, n이 커질 경우 파라미터 수가 너무 많아서 공간, 시간적 비용이 커짐
  - 희소 어텐션은 일부 키에 대해서만 어텐션 계산
  - 희소 어텐션 사용시 시퀀스 길이 n에 대해 연결의 수가 상대적으로 줄어들므로, 기존 밀집(dense) 어텐션보다 더 긴 시퀀스를 다룰 수 있게 됨
  - GPT-3에서는 희소 어텐션을 기존 어텐션과 번갈아 사용



### 3. 실험 방식

- 실험에 사용한 GPT-3 모델

  - GPT-2는 파라미터 수가 15억개(1.5B) 정도이므로, GPT-3 XL과 GPT-3 2.7B 사이에 위치

    <img src="./assets/Screenshot 2025-12-06 at 2.46.06 AM.png" alt="Screenshot 2025-12-06 at 2.46.06 AM" style="zoom:50%;" />

- 데이터셋

  - Common Crawl 데이터셋 사용
  - 웹 크롤링을 통해 모은 페타바이트 단위 데이터로 텍스트들의 총 단어 수 1조개
  - 모델의 크기가 커질수록 과적합을 막기 위해서는 더 많은 데이터가 필요하므로, 규모가 큰 텍스트 데이터 학습

- 프롬프트(prompt)

  - 언어 모델로 하여금 어떤 태스크를 수행하게 만들기 위해서, 또는 예측을 시키기 위해서는 적절히 지시하는 텍스트를 줘야 함
  - 프롬프트를 어떻게 제공하느냐에 따라 언어 모델이 만들어 내는 텍스트의 품질이 달라질 수 있음

- 프롬프트 제공 방식을 세 가지로 하여 모델을 평가

  - Zero-Shot
    - 예제를 하나도 주지 않고, 바로 답을 요구
  - One-Shot
    - 하나의 예를 제공하면서 답을 요구
  - Few-Shot
    - 몇 개의 예를 제공하면서 답을 요구



### 4. 실험 결과

<img src="./assets/Screenshot 2025-12-06 at 2.49.21 AM.png" alt="Screenshot 2025-12-06 at 2.49.21 AM" style="zoom:50%;" />

- Lambada 데이터셋
  - 지문을 읽고 문장의 마지막에 올 단어를 예측하는 태스크
  - 모델이 커질수록 성능이 향상되며, 특정 크기 이상에서 기존 SOTA 모델보다 좋은 성능
  - Few-Shot 방식으로 프롬프트 제공하는 것이 대체로 제일 좋은 성능 보임

- PhysicalQA 데이터셋

  <img src="./assets/Screenshot 2025-12-06 at 2.50.29 AM.png" alt="Screenshot 2025-12-06 at 2.50.29 AM" style="zoom:50%;" />

  - 상식에 대한 질문들에 답하는 태스크
  - 파라미터 수 증가할수록 정확도 상승
  - Few-Shot 세팅에서 가장 결과 좋음
  - 파라미터 수가 일정 수준 이상으로 커지면, 기존 SOTA 모델의 성능 능가

- Winogrande 데이터셋

  <img src="./assets/Screenshot 2025-12-06 at 2.52.00 AM.png" alt="Screenshot 2025-12-06 at 2.52.00 AM" style="zoom:50%;" />

  - 문장내 대명사가 무엇을 가리키는지 맞추는 태스크
  - 다른 태스크와 비슷한 양상 보였지만, 
  - 가장 큰 모델의 경우에도 SOTA 성능에는 미치지 못함

- 생성된 뉴스글을 이용한 평가

  <img src="./assets/Screenshot 2025-12-06 at 2.51.45 AM.png" alt="Screenshot 2025-12-06 at 2.51.45 AM" style="zoom:50%;" />

  - 모델이 생성해 낸 뉴스글을 사람에게 주고, 사람/모델 중 누가 쓴 글인지 맞추도록 함
  - 모델 크기가 커질수록 정확도가 50%에 가까워짐(구별 불가능)

- 결론
  - 몇몇 분야에서 SOTA 모델과 비슷하거나, 더 좋은 성능
  - Few-Shot 세팅에서 대체로 가장 좋은 성능
  - 충분한 데이터에 대해 학습을 수행한 초거대 언어 모델(Large Language Model)은 미세 조정 없이도 여러 분야의 태스크를 잘 수행할 수 있다는 것을 확인



## 5. ChatGPT

### 1. 아이디어

- GPT-3의 문제
  - GPT-3 만으로도 사람과 자연스럽게 대화할 수 있는 챗봇 만들 수 있음
  - 사실이 아닌 내용을 지어내는 경우 있음
  - 편향이 있거나, 무례한 답변을 만들어 냄
  - 모델에 내린 지시를 제대로 따르지 않을 수 있음
  - GPT 계열의 몯레이 학습한 방식이 웹에서 긁어온 텍스트를 학습해서 주어진 텍스트 다음에 올 자연스러운 단어를 맞추는 방식으로 이루어졌기 때문에, 모델의 목표는 사실을 전달하는 것이 아니라 '자연스럽게 보이는' 텍스트를 만들어 내는 것
  - GPT-3.5를 교정 → ChatGPT



### 2. 교정 방법

- Training Step 1
  - 데이터 셋에서 prompt 추출
  - 사람이 prompt에 대해 **바람직한 답안 생성**
  - 이렇게 만들어진 데이터를 가지고 미세조정 SFT(Supervised Fine-Tuning) 모델 생성
- Training Step 2
  - SFT가 prompt에 대한 여러 가지 답들을 생성해내게 함
  - 사람이 생성된 답에 대해 **순위(rank)를 매김**
  - 이렇게 만들어진 순위 데이터를 가지고 답에 대한 보상을 예측할 수 있는(즉, 사람이 해당 답을 어느 정도로 좋게 평가할지를 예측할 수 있는) RM(Reward Model)모델을 생성
- Training Step 3
  - SFT가 prompt에 대한 답을 생성하면, RM이 해당 답에 대한 보상 예측
  - 이 보상을 가지고 강화학습 알고리즘(PPO: Proximal Policy Optimization)을 적용해 SFT를 미세 조정한 모델을 생성(이 모델을 PPO)
  - 이와 같은 과정을 반복하면, RM이 SFT를 사람의 기준에 부합하도록 교정한 모델인 PPO를 만들어내게 됨
  - 이 방식을 RLHF(Reinforcement Learning from Human Feedback)



### 3. 평가

<img src="./assets/Screenshot 2025-12-06 at 2.59.48 AM.png" alt="Screenshot 2025-12-06 at 2.59.48 AM" style="zoom:50%;" />

- 진실성 및 유용성
  - 회색 막대: 진실성 정도
  - 유색 막대: 진실성과 유용성을 함께 평가한 척도
  - Instruction + QA prompt
    - 정확한 답을 모를 경우, 지어내지 말고 모른다고 대답하도록 지시를 추가한 경우
  - PPO(강화학습알고리즘)가 GPT에 비해 같은 사이즈의 모델을 비교했을 때 진실성 및 유용성 측면에서 더 높은 점수 

- Toxicity
  - 공격적, 혐오, 무례함 등을 포괄하는 개념
  - 존중에 관한 지시를 준 경우, GPT보다 SFT(미세조정)나 PPO(강화학습알고리즘)가 낮은 Toxicity를 보임
  - 존중에 대한 지시가 없는 경우에는 Human eval의 경우 SFT와 PPO가 GPT보다 낮은 Toxicity를 보임
  - API를 이용해 자동으로 평가한 경우, PPO가 GPT보다 특별히 더 나은 결과를 보이지 못함



### 4. 결론

- RLHF 방식으로 GPT-3의 문제 경감
  - 진실성, 유용성, Toxicity 면에서 모델 개선
  - ChatGPT는 기존 GPT-3 모델을 개선해 일반 유저들이 사용할 때 문제가 생길 여지를 줄인 모델
  - 평가 결과를 보면 문제가 완전히 해결된 것은 아님





## 6. [실습] 트랜스포머 기반 모델의 사용

### 1. 허깅페이스(Hugging Face)

- 딥러닝 모델 개발을 위한 각종 리소스 제공
  - Https://huggingface.co/
  - 트랜스포머 관련 라이브러리
  - 사전 학습된 모델
  - 딥러닝을 위한 각종 데이터셋
  - 평가 지표(evaluation metric)를 계산하기 위한 함수들
  - 문서들과 튜토리얼
  - 실제로 모델을 적용하는 경우, 사전 학습된 모델을 가져와 관심 분야의 목적에 맞게 미세 조정해서 사용하는 경우 많음



### 3. 결론

- 딥러닝 모델 개발의 실제: 전이 학습 방식의 활용
  - 관심 분야에 대해 사전 학습이 되어 있는 모델이 존재하는 경우,
  - 해당 모델 기반으로 필요한 모델 구현하는 것이 밑바닥에서부터 학습을 시작하는 것 보다 상대적으로 많은 비용 줄이고, 더 높은 성능을 기대할 수 있음
  - 허깅페이스에서 트랜스포머 관련 모델의 미세 조정을 위한 여러 가지 튜토리얼을 노트북 형태로 제공

